# main.py

from optimizer import PromptOptimizer
from logger import init_logger

def main():
    # Initialize log file
    init_logger()

    # === Specify the model ===
    model_name = "microsoft/phi-2"

    # Initialize optimizer
    optimizer = PromptOptimizer(model_name=model_name)

    # === Define task and data ===
    initial_prompt = "Classify this text as positive or negative."
    ground_truth = "positive"

    # Run optimization
    best_prompt, best_score = optimizer.optimize(
        initial_prompt=initial_prompt,
        ground_truth=ground_truth,
        num_rounds=5
    )

    # Final summary
    print("\n===== FINAL RESULTS =====")
    print(f"Best Prompt: {best_prompt}")
    print(f"Best Score: {best_score:.3f}")

if __name__ == "__main__":
    main()

# optimizer.py

import random
from logger import log_prompt
from models import TextModel

class PromptOptimizer:
    def __init__(self, model_name="microsoft/phi-2"):
        self.model_wrapper = TextModel(model_name)
        self.device = self.model_wrapper.device

    def score_prompt(self, prompt, ground_truth, model_output):
        """Dummy scoring function (replace later with real metrics)."""
        return random.uniform(0, 1)

    def reflect_on_prompt(self, prompt, score, feedback=""):
        """Rephrase the prompt based on reflection."""
        # Simple reflection: append feedback to the prompt
        return prompt + " " + feedback if feedback else prompt + " (refined)"

    def generate_output(self, prompt, input_text=""):
        """Generate model output for a given prompt."""
        full_prompt = prompt + " " + input_text if input_text else prompt
        return self.model_wrapper.generate(full_prompt)

    def optimize(self, initial_prompt, ground_truth, num_rounds=5):
        """Optimize the prompt over multiple rounds."""
        current_prompt = initial_prompt
        best_prompt = current_prompt
        best_score = 0

        for round_num in range(1, num_rounds + 1):
            # Candidate transformation
            transformation = "initial" if round_num == 1 else "reflection"

            # Generate output and score
            output = self.generate_output(current_prompt)
            score = self.score_prompt(current_prompt, ground_truth, output)

            # Log the prompt evaluation
            candidate_id = round_num  # simple ID for each candidate
            parent_prompt = current_prompt
            log_prompt(round_num, candidate_id, parent_prompt, transformation, current_prompt, score)

            # Reflect to improve prompt
            current_prompt = self.reflect_on_prompt(current_prompt, score)

            # Update best prompt if score improves
            if score > best_score:
                best_score = score
                best_prompt = current_prompt

            print(f"=== Round {round_num} ===")
            print(f"Prompt: {current_prompt}")
            print(f"Score: {score:.3f}")

        return best_prompt, best_score

# models.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class TextModel:
    def __init__(self, model_name="microsoft/phi-2"):
        print(f"Loading model: {model_name}")
        # Set device: MPS if available, otherwise CPU
        self.device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Load model in full precision for stability
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)
        print(f"Model loaded on device: {self.device} (float32 for stability)")

    def generate(self, prompt, max_length=100):
        # Tokenize input and send tensors to the correct device
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # Generate output safely
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode and return
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

import csv
import json
import os
from datetime import datetime

CSV_FILE = "prompt_logs.csv"
JSONL_FILE = "prompt_logs.jsonl"

def init_logger():
    if not os.path.exists(CSV_FILE):
        with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["timestamp","round","candidate_id","parent_prompt","transformation","prompt","score","delta"])

def log_prompt(round_num, candidate_id, parent_prompt, transformation, prompt, score, delta=None):
    ts = datetime.utcnow().isoformat()
    row = [ts, round_num, candidate_id, parent_prompt, transformation, prompt, score, delta]
    with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(row)
    # JSONL
    with open(JSONL_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps({
            "timestamp": ts,
            "round": round_num,
            "candidate_id": candidate_id,
            "parent_prompt": parent_prompt,
            "transformation": transformation,
            "prompt": prompt,
            "score": score,
            "delta": delta
        }) + "\n")

